{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_FfV28rJXrvmURl66380upn0gZp6tOzT",
      "authorship_tag": "ABX9TyOGenZUJwFJpOuVssW1b40G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/klnsuman/LangChain/blob/main/Agents_SerpAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qe9riKvh_KFO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz_cr-1A_M6Y",
        "outputId": "5f2eee0f-9f6f-4653-c59d-e07e2c1a85f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/colab_env/lib/python3.10/site-packages\")\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV3JZUPl_VT0",
        "outputId": "eb6db723-7c4b-4639-dd31-5c9758efe7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "u2uvZ3z0Bh1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "\n",
        "# import os\n",
        "# os.environ['OPENAI_API_KEY'] = \"...\"\n",
        "# os.environ['SERPAPI_API_KEY'] = \"...\""
      ],
      "metadata": {
        "id": "wcdsvNjTMGjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['SERPAPI_API_KEY'] = getpass.getpass('OpenAI API Key:')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwyUDfF2I9UW",
        "outputId": "5b5252f2-9579-44d8-f2be-8cb8515df1ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "fY6gqdgQJJvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9_Wz_OzMjLP",
        "outputId": "1d568833-d8f2-4689-eb48-de468a24a693"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2023.7.22)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32003 sha256=c17ea6648246026149eed8f5ea385dc67ff5c8da251b10eabe6d9a895aa99daf\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tool_names = [\"serpapi\"]\n",
        "tools = load_tools(tool_names)\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "agent.run(\"What is LangChain?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "B6M7FPKXMbK4",
        "outputId": "1efe5d12-8f5f-4030-f3f0-2b636ce51f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should research this online.\n",
            "Action: Search\n",
            "Action Input: \"LangChain\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m[\"LangChain is a framework designed to simplify the creation of applications using large language models. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\", 'LangChain kgmid: /g/11kjpl7_60.', 'LangChain developer_s: Harrison Chase.', 'LangChain initial_release: October 2022.', 'LangChain license: MIT License.', 'LangChain written_in: Python and JavaScript.', 'LangChain ¬∑ A complete set of powerful building blocks ¬∑ Smart connections to any source of data or knowledge ¬∑ Easily-swappable components ¬∑ A developer ...']\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m LangChain is a language model integration framework.\n",
            "Final Answer: LangChain is a framework designed to simplify the creation of applications using large language models. It is written in Python and JavaScript and was initially released in October 2022 under the MIT License. It is developed by Harrison Chase.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain is a framework designed to simplify the creation of applications using large language models. It is written in Python and JavaScript and was initially released in October 2022 under the MIT License. It is developed by Harrison Chase.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input should be a search query.\n",
        "agent.run(\"who is the ceo of pipe?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "Ebb3SB4pNrnO",
        "outputId": "7ed9b85d-b68e-4012-d4da-531bda222938"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out who the CEO of Pipe is.\n",
            "Action: Search\n",
            "Action Input: \"ceo of pipe\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mLuke Voiles\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Luke Voiles is the CEO of Pipe.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Luke Voiles is the CEO of Pipe.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Summarization***"
      ],
      "metadata": {
        "id": "oHquEX2r-4_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Note, the default model is already 'text-davinci-003' but I call it out here explicitly so you know where to change it later if you want\n",
        "llm = OpenAI(temperature=0, model_name='text-davinci-003')\n",
        "\n",
        "# Create our template\n",
        "template = \"\"\"\n",
        "%INSTRUCTIONS:\n",
        "Please summarize the following piece of text.\n",
        "Respond in a manner that a 5 year old would understand.\n",
        "\n",
        "%TEXT:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "# Create a LangChain prompt template that we can insert values to later\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=template,\n",
        ")"
      ],
      "metadata": {
        "id": "iNgl33l4OVDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusing_text = \"\"\"\n",
        "For the next 130 years, debate raged.\n",
        "Some scientists called Prototaxites a lichen, others a fungus, and still others clung to the notion that it was some kind of tree.\n",
        "‚ÄúThe problem is that when you look up close at the anatomy, it‚Äôs evocative of a lot of different things, but it‚Äôs diagnostic of nothing,‚Äù says Boyce, an associate professor in geophysical sciences and the Committee on Evolutionary Biology.\n",
        "‚ÄúAnd it‚Äôs so damn big that when whenever someone says it‚Äôs something, everyone else‚Äôs hackles get up: ‚ÄòHow could you have a lichen 20 feet tall?‚Äô‚Äù\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bhxADl5RDkk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"------- Prompt Begin -------\")\n",
        "\n",
        "final_prompt = prompt.format(text=confusing_text)\n",
        "print(final_prompt)\n",
        "\n",
        "print (\"------- Prompt End -------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IP3BQ21cDpYn",
        "outputId": "69c9b01a-5ec2-4971-a9d6-0223e8612375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------- Prompt Begin -------\n",
            "\n",
            "%INSTRUCTIONS:\n",
            "Please summarize the following piece of text.\n",
            "Respond in a manner that a 5 year old would understand.\n",
            "\n",
            "%TEXT:\n",
            "\n",
            "For the next 130 years, debate raged.\n",
            "Some scientists called Prototaxites a lichen, others a fungus, and still others clung to the notion that it was some kind of tree.\n",
            "‚ÄúThe problem is that when you look up close at the anatomy, it‚Äôs evocative of a lot of different things, but it‚Äôs diagnostic of nothing,‚Äù says Boyce, an associate professor in geophysical sciences and the Committee on Evolutionary Biology.\n",
            "‚ÄúAnd it‚Äôs so damn big that when whenever someone says it‚Äôs something, everyone else‚Äôs hackles get up: ‚ÄòHow could you have a lichen 20 feet tall?‚Äô‚Äù\n",
            "\n",
            "\n",
            "------- Prompt End -------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(final_prompt)\n",
        "print (output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BCqM0LvFSV8",
        "outputId": "e9a5c80a-dfac-4407-c0cf-b010f60fd1ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "For 130 years, people argued about what Prototaxites was. Some thought it was a lichen, some thought it was a fungus, and some thought it was a tree. But no one could agree. It was so big that it was hard to figure out what it was.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Summaries of Longer Text***"
      ],
      "metadata": {
        "id": "cn3KKs8BJqcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "jfs925pdFVRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/good.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Printing the first 285 characters as a preview\n",
        "print (text[:285])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvWxZoGzKDOM",
        "outputId": "d95431eb-0bae-4831-863c-988b4a171a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "April 2008(This essay is derived from a talk at the 2008 Startup School.)About a month after we started Y Combinator we came up with the\n",
            "phrase that became our motto: Make something people want.  We've\n",
            "learned a lot since then, but if I were choosing now that's still\n",
            "the one I'd pick.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = llm.get_num_tokens(text)\n",
        "\n",
        "print (f\"There are {num_tokens} tokens in your file\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Wmc9hstKt14",
        "outputId": "6bb2d8a5-8e46-4653-b3b3-32dddeb90388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 3970 tokens in your file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\\n\"], chunk_size=5000, chunk_overlap=350)\n",
        "docs = text_splitter.create_documents([text])\n",
        "\n",
        "print (f\"You now have {len(docs)} docs intead of 1 piece of text\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgfoAh-LKxIG",
        "outputId": "7d273cf6-9d6a-46b8-fca7-05f46ea56312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You now have 4 docs intead of 1 piece of text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your chain ready to use\n",
        "chain = load_summarize_chain(llm=llm, chain_type='map_reduce') # verbose=True optional to see what is getting sent to the LLM"
      ],
      "metadata": {
        "id": "0NsmEoJkLUFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use it. This will run through the 4 documents, summarize the chunks, then get a summary of the summary.\n",
        "output = chain.run(docs)\n",
        "print (output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTstS8B0LV48",
        "outputId": "2556c60f-79bb-4175-8b43-6206c546f0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " This essay discusses the importance of benevolence in startups, and how it can help them succeed. It explains how benevolence can improve morale, make people want to help, and help startups be decisive. It also looks at how markets have evolved to value potential dividends and potential earnings, and how starting a company with benevolent aims is currently undervalued. Y Combinator's motto of \"Make something people want\" is used as an example of how this principle can be applied to many different types of startups.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Question And Answering***"
      ],
      "metadata": {
        "id": "Hrw3QIzCLhNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "N733VOfULgtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\"\n",
        "Rachel is 30 years old\n",
        "Bob is 45 years old\n",
        "Kevin is 65 years old\n",
        "\"\"\"\n",
        "\n",
        "question = \"Who is under 40 years old?\""
      ],
      "metadata": {
        "id": "qzX0irJSNcuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(context + question)\n",
        "\n",
        "# I strip the text to remove the leading and trailing whitespace\n",
        "print (output.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWUlV_P7NgBO",
        "outputId": "4e80f29c-e7dd-4f4b-ebfc-75985c983a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rachel is under 40 years old.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Using Embeddings in Q&A***"
      ],
      "metadata": {
        "id": "-Q9jkLMHOUTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI\n",
        "\n",
        "# The vectorstore we'll be using\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# The LangChain component we'll use to get the documents\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# The easy document loader for text\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# The embedding engine that will convert our text to vectors\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "832ZF9XjNjc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = TextLoader('/content/drive/MyDrive/Data/worked_1.txt')\n",
        "doc = loader.load()\n",
        "print (f\"You have {len(doc)} document\")\n",
        "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdJtePJwOfF2",
        "outputId": "00524a57-bb52-4e68-e8ce-f172f0ced7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 1 document\n",
            "You have 74663 characters in that document\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
        "docs = text_splitter.split_documents(doc)"
      ],
      "metadata": {
        "id": "iGaqhiUgOmYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the total number of characters so we can see the average later\n",
        "num_total_characters = sum([len(x.page_content) for x in docs])\n",
        "\n",
        "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters (smaller pieces)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuUnF6YcOo94",
        "outputId": "23865901-8a2a-4bad-df16-dcc9028a4c5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now you have 29 documents that have an average of 2,930 characters (smaller pieces)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your embeddings engine ready\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Embed your documents and combine with the raw text in a pseudo db. Note: This will make an API call to OpenAI\n",
        "docsearch = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "yrB9gkVROvyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
      ],
      "metadata": {
        "id": "DPoUAt-KO-bK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What does the author describe as good work?\"\n",
        "qa.run(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DTPdzZoTPBcD",
        "outputId": "d44fbee0-765e-4e3e-b973-b3f9ebb20ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The author describes painting as good work.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Extraction***"
      ],
      "metadata": {
        "id": "7xZ5S3_iPI33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To help construct our Chat Messages\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# We will be using a chat model, defaults to gpt-3.5-turbo\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# To parse outputs and get structured data back\n",
        "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
        "\n",
        "chat_model = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')"
      ],
      "metadata": {
        "id": "dlbaXLenPIJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = \"\"\"\n",
        "You will be given a sentence with fruit names, extract those fruit names and assign an emoji to them\n",
        "Return the fruit name and emojis in a python dictionary\n",
        "\"\"\"\n",
        "\n",
        "fruit_names = \"\"\"\n",
        "Apple, Pear, this is an kiwi\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Lp_Lfh15UzFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make your prompt which combines the instructions w/ the fruit names\n",
        "prompt = (instructions + fruit_names)\n",
        "\n",
        "# Call the LLM\n",
        "output = chat_model([HumanMessage(content=prompt)])\n",
        "\n",
        "print (output.content)\n",
        "print (type(output.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1h6GVOQNU-tY",
        "outputId": "a848dd45-24a8-4d80-ee00-ecc7ce1c0994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"Apple\": \"üçé\",\n",
            "  \"Pear\": \"üçê\",\n",
            "  \"kiwi\": \"ü•ù\"\n",
            "}\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dict = eval(output.content)\n",
        "\n",
        "print (output_dict)\n",
        "print (type(output_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAtA9KE9VA_g",
        "outputId": "62daf5cb-ac29-4f3e-ca18-93ec65909dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Apple': 'üçé', 'Pear': 'üçê', 'kiwi': 'ü•ù'}\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The schema I want out\n",
        "response_schemas = [\n",
        "    ResponseSchema(name=\"artist\", description=\"The name of the musical artist\"),\n",
        "    ResponseSchema(name=\"song\", description=\"The name of the song that the artist plays\")\n",
        "]\n",
        "\n",
        "# The parser that will look for the LLM output in my schema and return it back to me\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
      ],
      "metadata": {
        "id": "Aous9a3aaOPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The format instructions that LangChain makes. Let's look at them\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4r0x2t6pADn",
        "outputId": "5269af08-bc33-4242-9322-45867efb9923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"artist\": string  // The name of the musical artist\n",
            "\t\"song\": string  // The name of the song that the artist plays\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The prompt template that brings it all together\n",
        "# Note: This is a different prompt template than before because we are using a Chat Model\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        HumanMessagePromptTemplate.from_template(\"Given a command from the user, extract the artist and song names \\n \\\n",
        "                                                    {format_instructions}\\n{user_prompt}\")\n",
        "    ],\n",
        "    input_variables=[\"user_prompt\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")"
      ],
      "metadata": {
        "id": "SB3A-aGRpGiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fruit_query = prompt.format_prompt(user_prompt=\"I really like So Young by Portugal. The Man\")\n",
        "print (fruit_query.messages[0].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NLLSb0ApHyz",
        "outputId": "c0421524-20da-4aa2-b835-ebe609b42499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given a command from the user, extract the artist and song names \n",
            "                                                     The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"artist\": string  // The name of the musical artist\n",
            "\t\"song\": string  // The name of the song that the artist plays\n",
            "}\n",
            "```\n",
            "I really like So Young by Portugal. The Man\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fruit_output = chat_model(fruit_query.to_messages())\n",
        "output = output_parser.parse(fruit_output.content)\n",
        "\n",
        "print (output)\n",
        "print (type(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64WYduuNpLqd",
        "outputId": "9df48e88-b0d8-4c1f-f354-cca83f9406f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'artist': 'Portugal. The Man', 'song': 'So Young'}\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Evaluation***"
      ],
      "metadata": {
        "id": "dI1QhtnywaGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings, store, and retrieval\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Model and doc loader\n",
        "from langchain import OpenAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# Eval!\n",
        "from langchain.evaluation.qa import QAEvalChain\n",
        "\n",
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "bUrdHOl9pPpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our long essay from before\n",
        "loader = TextLoader('/content/drive/MyDrive/Data/worked.txt')\n",
        "doc = loader.load()\n",
        "\n",
        "print (f\"You have {len(doc)} document\")\n",
        "print (f\"You have {len(doc[0].page_content)} characters in that document\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iEjW_Lyx_E1",
        "outputId": "7233d20b-1454-4594-fa10-a41075b837da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have 1 document\n",
            "You have 74663 characters in that document\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=400)\n",
        "docs = text_splitter.split_documents(doc)\n",
        "\n",
        "# Get the total number of characters so we can see the average later\n",
        "num_total_characters = sum([len(x.page_content) for x in docs])\n",
        "\n",
        "print (f\"Now you have {len(docs)} documents that have an average of {num_total_characters / len(docs):,.0f} characters (smaller pieces)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apYB8KBFyKhd",
        "outputId": "73ce94fc-a04e-4f7c-f875-871ce3aca24e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now you have 29 documents that have an average of 2,930 characters (smaller pieces)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings and docstore\n",
        "embeddings = OpenAIEmbeddings()\n",
        "docsearch = FAISS.from_documents(docs, embeddings)"
      ],
      "metadata": {
        "id": "bDc26Ch7yV1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docsearch.as_retriever(), input_key=\"question\")"
      ],
      "metadata": {
        "id": "ey8Phv1jyhw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_answers = [\n",
        "    {'question' : \"Which company sold the microcomputer kit that his friend built himself?\", 'answer' : 'Healthkit'},\n",
        "    {'question' : \"What was the small city he talked about in the city that is the financial capital of USA?\", 'answer' : 'Yorkville, NY'}\n",
        "]"
      ],
      "metadata": {
        "id": "m7CB6v0JyymA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = chain.apply(question_answers)\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MoXQUgCy19M",
        "outputId": "809b53da-3f4c-4523-8d47-37529bb7e408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'question': 'Which company sold the microcomputer kit that his friend built himself?',\n",
              "  'answer': 'Healthkit',\n",
              "  'result': ' The microcomputer kit was sold by Heathkit.'},\n",
              " {'question': 'What was the small city he talked about in the city that is the financial capital of USA?',\n",
              "  'answer': 'Yorkville, NY',\n",
              "  'result': ' The small city he talked about is New York City, the financial capital of the USA.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start your eval chain\n",
        "eval_chain = QAEvalChain.from_llm(llm)\n",
        "\n",
        "# Have it grade itself. The code below helps the eval_chain know where the different parts are\n",
        "graded_outputs = eval_chain.evaluate(question_answers,\n",
        "                                     predictions,\n",
        "                                     question_key=\"question\",\n",
        "                                     prediction_key=\"result\",\n",
        "                                     answer_key='answer')"
      ],
      "metadata": {
        "id": "vsixjR3XzSAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graded_outputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCxtme09zTEd",
        "outputId": "81b27cfb-a5e8-430c-d0ad-afafbd346586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'results': ' CORRECT'}, {'results': ' INCORRECT'}]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Querying Tabular Data***"
      ],
      "metadata": {
        "id": "lXqAz3gb0wKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U langchain langchain_experimental"
      ],
      "metadata": {
        "id": "PwTBz3rwI5TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, SQLDatabase\n",
        "#from langchain_experimental.sql import SQLDatabaseChain\n",
        "from langchain_experimental.sql import SQLDatabaseChain\n",
        "\n",
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "P_QzAlSa01ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sqlite_db_path = '/content/drive/MyDrive/Data/San_Francisco_Trees.db'\n",
        "db = SQLDatabase.from_uri(f\"sqlite:///{sqlite_db_path}\")"
      ],
      "metadata": {
        "id": "rdc9ZtTdzVXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_chain = SQLDatabaseChain(llm=llm, database=db, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naQ2aT7tJ0mF",
        "outputId": "7e77679d-f763-47fe-b32a-adf8758e589c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_experimental/sql/base.py:75: UserWarning: Directly instantiating an SQLDatabaseChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db_chain.run(\"How many Species of trees are there in San Francisco?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "HkQhx_gEJ5RV",
        "outputId": "5d70f788-b493-40d6-a3b3-64bb5fe3f4e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "How many Species of trees are there in San Francisco?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT COUNT(DISTINCT \"qSpecies\") FROM \"SFTrees\";\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[(578,)]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mThere are 578 Species of trees in San Francisco.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are 578 Species of trees in San Francisco.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('product_db.db')\n",
        "c = conn.cursor()\n",
        "\n",
        "c.execute('''\n",
        "          CREATE TABLE IF NOT EXISTS products\n",
        "          (\n",
        "            [id] INTEGER PRIMARY KEY,\n",
        "            [name] TEXT,\n",
        "            [supplier] TEXT,\n",
        "            [store] TEXT\n",
        "          )\n",
        "          ''')\n",
        "\n",
        "c.execute('''\n",
        "          CREATE TABLE IF NOT EXISTS prices\n",
        "          (\n",
        "              [id] INTEGER PRIMARY KEY,\n",
        "              [price] INTEGER,\n",
        "              [discount_pcnt] INTEGER\n",
        "          )\n",
        "          ''')\n",
        "sql = ''' INSERT INTO products(id,name,supplier,store)\n",
        "              VALUES(1,'IPhone','Apple','Hyderabad') '''\n",
        "sql = ''' INSERT INTO products(id,name,supplier,store)\n",
        "VALUES(2,'IPod','Apple','Knr') '''\n",
        "sql = ''' INSERT INTO products(id,name,supplier,store)\n",
        "VALUES(3,'Tablet','Samsung','Hnk') '''\n",
        "sql = ''' INSERT INTO products(id,name,supplier,store)\n",
        "VALUES(4,'Desk','Dell','Knr') '''\n",
        "sql = ''' INSERT INTO products(id,name,supplier,store)\n",
        "VALUES(5,'Macbook','Apple','Hyderabad') '''\n",
        "\n",
        "\n",
        "\n",
        "c.execute(sql)\n",
        "conn.commit()\n",
        "conn.commit()"
      ],
      "metadata": {
        "id": "sBWskub6P3b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "conn = sqlite3.connect('test.db')\n",
        "print(\"Opened database successfully\")\n",
        "conn.execute('''CREATE TABLE COMPANY\n",
        "         (ID INT PRIMARY KEY     NOT NULL,\n",
        "         NAME           TEXT    NOT NULL,\n",
        "         AGE            INT     NOT NULL,\n",
        "         ADDRESS        CHAR(50),\n",
        "         SALARY         REAL);''')\n",
        "print(\"Table created successfully\");\n",
        "\n",
        "conn.execute('''INSERT INTO COMPANY (ID,NAME,AGE,ADDRESS,SALARY)\n",
        "      VALUES (1, 'Paul', 32, 'California', 20000.00 )''')\n",
        "\n",
        "conn.execute('''INSERT INTO COMPANY (ID,NAME,AGE,ADDRESS,SALARY)\n",
        "      VALUES (2, 'Allen', 25, 'Texas', 15000.00 )''')\n",
        "\n",
        "conn.execute('''INSERT INTO COMPANY (ID,NAME,AGE,ADDRESS,SALARY)\n",
        "      VALUES (3, 'Teddy', 23, 'Norway', 20000.00 )''')\n",
        "conn.execute('''INSERT INTO COMPANY (ID,NAME,AGE,ADDRESS,SALARY)\n",
        "      VALUES (4, 'Mark', 25, 'Rich-Mond ', 65000.00 )''')\n",
        "\n",
        "conn.commit()\n",
        "print(\"Records created successfully\")\n",
        "conn.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yJBfnEsP5o4",
        "outputId": "7b3203a7-a71a-4357-fd89-39f8ebe73e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opened database successfully\n",
            "Table created successfully\n",
            "Records created successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db = SQLDatabase.from_uri(\"sqlite:///test.db\")"
      ],
      "metadata": {
        "id": "yN4xzIMET4M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)"
      ],
      "metadata": {
        "id": "ujcqJFNjUA8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db_chain.run(\"How many company are there ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "IYifXbFIUF70",
        "outputId": "94d76d3e-7841-488e-d30a-8d07ffc594ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "How many company are there ?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT COUNT(*) FROM \"COMPANY\";\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[(4,)]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mThere are 4 companies.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are 4 companies.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db_chain.run(\"What is maximum salary?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "wvq1n29cUiL_",
        "outputId": "c4060eaf-2d57-44ba-a7c5-be148aa7b5d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SQLDatabaseChain chain...\u001b[0m\n",
            "What is maximum salary?\n",
            "SQLQuery:\u001b[32;1m\u001b[1;3mSELECT MAX(\"SALARY\") FROM \"COMPANY\";\u001b[0m\n",
            "SQLResult: \u001b[33;1m\u001b[1;3m[(65000.0,)]\u001b[0m\n",
            "Answer:\u001b[32;1m\u001b[1;3mThe maximum salary is 65000.0.\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The maximum salary is 65000.0.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Chat Bots***"
      ],
      "metadata": {
        "id": "1dOH8BvFY7lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "\n",
        "# Chat specific components\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "glIAheFSY7Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "You are a chatbot that is unhelpful.\n",
        "Your goal is to not help the user but only make jokes.\n",
        "Take what the user is saying and make a joke out of it\n",
        "\n",
        "{chat_history}\n",
        "Human: {human_input}\n",
        "Chatbot:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"human_input\"],\n",
        "    template=template\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
      ],
      "metadata": {
        "id": "cypkhyEwmpk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(\n",
        "    llm=OpenAI(),\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")"
      ],
      "metadata": {
        "id": "ZZEIcNtdnfDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(human_input=\"Is an pear a fruit or vegetable?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "cYbntWYpopEx",
        "outputId": "6a2123af-0763-4a5f-c0e2-316b2f6ff3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "You are a chatbot that is unhelpful.\n",
            "Your goal is to not help the user but only make jokes.\n",
            "Take what the user is saying and make a joke out of it\n",
            "\n",
            "Human: Is an pear a fruit or vegetable?\n",
            "AI:  It would be fruit-ful to say it's a fruit!\n",
            "Human: Is an pear a fruit or vegetable?\n",
            "Chatbot:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Depends, are you trying to make a salad or a pie?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(human_input=\"What was one of the fruits I first asked you about?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "6q6_X1DFoxRr",
        "outputId": "931ca3d0-1465-45cf-e2cd-7a6003426666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "You are a chatbot that is unhelpful.\n",
            "Your goal is to not help the user but only make jokes.\n",
            "Take what the user is saying and make a joke out of it\n",
            "\n",
            "Human: Is an pear a fruit or vegetable?\n",
            "AI:  It would be fruit-ful to say it's a fruit!\n",
            "Human: Is an pear a fruit or vegetable?\n",
            "AI:  Depends, are you trying to make a salad or a pie?\n",
            "Human: What was one of the fruits I first asked you about?\n",
            "Chatbot:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' An pear of course!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Agents***"
      ],
      "metadata": {
        "id": "Tm7j4VOrruMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpers\n",
        "import os\n",
        "import json\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Agent imports\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "# Tool imports\n",
        "from langchain.agents import Tool\n",
        "from langchain.utilities import GoogleSearchAPIWrapper\n",
        "from langchain.utilities import TextRequestsWrapper"
      ],
      "metadata": {
        "id": "N-NC7RpAo_7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "QdsjnBCIFmP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search = GoogleSearchAPIWrapper(google_api_key=GOOGLE_API_KEY, google_cse_id=GOOGLE_CSE_ID)\n",
        "\n",
        "requests = TextRequestsWrapper()"
      ],
      "metadata": {
        "id": "PV49js9lFrB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toolkit = [\n",
        "    Tool(\n",
        "        name = \"Search\",\n",
        "        func=search.run,\n",
        "        description=\"useful for when you need to search google to answer questions about current events\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name = \"Requests\",\n",
        "        func=requests.get,\n",
        "        description=\"Useful for when you to make a request to a URL\"\n",
        "    ),\n",
        "]\n"
      ],
      "metadata": {
        "id": "SGK3SaOtGb6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)"
      ],
      "metadata": {
        "id": "eymohQrAGfA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent({\"input\":\"What is the capital of canada?\"})\n",
        "response['output']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "ptAVRDObGiZA",
        "outputId": "34d8b2e3-d135-4f19-d3eb-141e625118f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out what the capital of Canada is.\n",
            "Action: Search\n",
            "Action Input: \"capital of Canada\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mLooking to build credit or earn rewards? Compare our rewards, Guaranteed secured and other Guaranteed credit cards. Canada's capital is Ottawa and its three largest metropolitan areas are Toronto, Montreal, and Vancouver. Canada. A vertical triband design (red, white, red)¬†... Ottawa, city, capital of Canada, located in southeastern Ontario. In the eastern extreme of the province, Ottawa is situated on the Ottawa River across from¬†... Ottawa is the capital city of Canada. It is located in the southern portion of the province of Ontario, at the confluence of the Ottawa River and the Rideau¬†... As digital continues to change how we live and bank, our goal is to bring ingenuity, simplicity and humanity to banking. Together, we will build one of Canada's¬†... Read reviews, compare customer ratings, see screenshots and learn more about Capital One Canada. Download Capital One Canada and enjoy it on your iPhone,¬†... Aug 17, 2023 ... Residents in the capital of Canada's Northwest Territories were ordered to evacuate Wednesday night as wildfires neared the city of 20000¬†... Ottawa is Canada's capital city and has long been known as an important hub for commerce and travel. In fact, Ottawa's name comes from the Algonquin word¬†... Canada. United States. Fiera Capital. Visit the site Fiera Capital, United ... Fiera Capital Canadian Equities Responsible Investing Report. Insights. Nov 23,¬†... With us, you're not just a customer, you're a member. 500000+ Members ¬∑ Low Mortgage Rates ¬∑ Online Banking ¬∑ Friendly Service ¬∑ Tools and Calculators¬†...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: Ottawa is the capital of Canada.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ottawa is the capital of Canada.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/colab_env/lib/python3.10/site-packages\")\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cafte0bmJifn",
        "outputId": "d9a72cc5-7e86-4f3a-e18d-e00a540a20ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain"
      ],
      "metadata": {
        "id": "fU7r8tnjKbNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install unstructured"
      ],
      "metadata": {
        "id": "DVfm7nzGKfbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tq8DDN2bNHsI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}